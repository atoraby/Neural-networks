{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras \n",
    "\n",
    "Keras is an API that sits on top of Googleâ€™s TensorFlow, Microsoft Cognitive Toolkit (CNTK), and other machine learning frameworks. The goal is to have a single API to work with all of those and to make that work easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras we have some data sets that can be used to train neural networks. For example mnist data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images,train_labels),(test_images, test_labels)=fashion_mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 60000 images for traning and 10000 images for testing. Each image is a matrix of 28 x 28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_images.shape)\n",
    "print(train_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The images have different labels. Here is the list of label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = train_images[0]\n",
    "sample2 = train_images[1]\n",
    "sample3 = train_images[2]\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize = (5,5))\n",
    "ax1.imshow(sample1)\n",
    "ax2.imshow(sample2)\n",
    "ax3.imshow(sample3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The data that is used to train a neural network usually needs to be processed. For this example, we need to change the range of the value for each pixel from 0 to 255 to 0..1. This helps our NN to fit the data more easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255\n",
    "test_images = test_images / 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    " Next we create the model using keras Sequential. This gives us a sequential neural network. In the following code, we create a model that has 784 input nodes. In the next layer we have 120 nodes that are densely connected by the nodes in the previous layer. In the output layer there are 10 nodes that stand for the ten labels of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(120,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10,activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the structure of the model, we need to set other parameters. The loss function, the optimizer and the metrics using the compile method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\" , loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Finally we use fit method to train our model.\n",
    "\n",
    "In the following example, we used batch size = 100, meaning that in each learning loop keras runs forward algorithm on 100 samples and then calculates the loss to update the gradient. Therefore, for 60000 samples we run the learning loop 600 times. We run this training session 10 times. Overal we update the weights 6000 times.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images , train_labels , epochs=10 , batch_size=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model gets better after each batch and and each epoch. However, we can also cancel the fitting process eralier using a kind of cancellation method!\n",
    "\n",
    "The following code shows how we can use a callback function to cancel the training process when the loss value reached a specific threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get(\"loss\")< 0.4):\n",
    "            print(\"\\nReached 0.,6 accuracy\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "loss = LossCallback()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(120,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10,activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\" , loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])\n",
    "model.fit(train_images , train_labels , epochs=10 , batch_size=100 , callbacks=[loss])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "Now that we created a model that has accuracy of 90 percent on our training model, we need to evaluate it on our test data set. Because this is the accuracy on the test data set that really matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images,test_labels , verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy on the testing data set is lower, which means that our model is kind of overfitting over training data. So we should try other parameters and test the model again. For example, I think that if I reduce the value for epochs it would be a good idea to avoid overfiting. In the following code I use epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images , train_labels , epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images,test_labels , verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy on test data is higher afterusging epoch = 2. This process is called hyper parameter tuning. We have to use the best options and this is done by experience and knowledge. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "We can use the model the predict the label of new images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_images[20]\n",
    "predictions = model.predict(sample)\n",
    "print(predictions)\n",
    "print(labels[np.argmax(predictions)])\n",
    "fig , axes = plt.subplots(1,1,figsize=(3,3))\n",
    "axes.imshow(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nn-nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a01f484bbe5ff7db30bde6dc76d21cf72867a81df06775fe6a282b1f2bc3b0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

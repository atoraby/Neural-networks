{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "\n",
    "## Probabilistic modeling\n",
    "Probabilistic modeling is the application of the principles of statistics to data analysis. It is one of the earliest forms of machine learning, and it’s still widely used to this day. One of the best-known algorithms in this category is the Naive Bayes algorithm.\n",
    "\n",
    "Logistic regression is the other statistic tool which is still popular these days.\n",
    "\n",
    "## Kernel methods\n",
    "Kernel methods are a group of classification algorithms, the best known of which is the Support Vector Machine (SVM).\n",
    "At the time they were developed, SVMs exhibited state-of-the-art performance on simple classification problems and were one of the few machine learning methods backed by extensive theory and amenable to serious mathematical analysis, making them well understood and easily interpretable. Because of these useful properties, SVMs became extremely popular in the field for a long time.\n",
    "\n",
    "But SVMs proved hard to scale to large datasets and didn’t provide good results for perceptual problems such as image classification. Because an SVM is a shallow method, applying an SVM to perceptual problems requires first extracting useful representations manually (a step called feature engineering), which is difficult and brittle. For instance, if you want to use an SVM to classify handwritten digits, you can’t start from the raw pixels; you should first find by hand useful representations that make the problem more tractable, like the pixel histograms mentioned earlier.\n",
    "\n",
    "## Decision trees, random forests, and gradient boosting machines\n",
    "Decision trees are flowchart-like structures that let you classify input data points or predict output values given inputs. They’re easy to visualize and interpret. Decision trees learned from data began to receive significant research interest in the 2000s, and by 2010 they were often preferred to kernel methods.\n",
    "\n",
    "The Random Forest algorithm introduced a robust, practical take on decision-tree learning that involves building a large number of specialized decision trees and then ensembling their outputs. Random forests are applicable to a wide range of problems—you could say that they’re almost always the second-best algorithm for any shallow machine learning task.\n",
    "\n",
    "The gradient boosting technique results in models that strictly outperform random forests most of the time, while having similar properties. It may be one of the best, if not the best, algorithm for dealing with nonperceptual data today.\n",
    "\n",
    "## Deep learning\n",
    "Deep learning is a specific subset of machine learning. It puts and an emphasis on learning successive layers of increasingly meaningful representations of the data. In deep learning the learning is done by a stack of neural network layers that are attached on the top of each other that try to represent the data in a form that is amenable for simple rules to make the final decisions.\n",
    "\n",
    "\n",
    "<h3>What makes deep learning different</h3>\n",
    "The primary reason deep learning took off so quickly is that it offered better performance for many problems. But that’s not the only reason. Deep learning also makes problem-solving much easier, because it completely automatesfeature engineering which used to be the most crucial step in a machine learning workflow before deep learning.\n",
    "\n",
    "For example, when we use neural netowrks for image processing, each layer transforms the input image to a form that is increasingly different from the original input image and more pure and informative about the final result. \n",
    "\n",
    "<img src=\"images/deep_learning.png\" width=\"50%\">\n",
    "\n",
    "The above picture illustrates that classifing hand written digits can be done by four simple rules using the last layer of a neural network, while a rule-based system may need hundreds of rules to classify digits based on the original image.\n",
    "\n",
    "These layers have been named in literature:\n",
    "\n",
    "1.   Input layer: The first layer is called the input layer. The number of nodes in the input layer is rela†ed to the number of the features in our dataset.\n",
    "2.   Output layer: The final layer is called the output layer. this layer may have only one node which indicates the final out put. For example a range between 0 and 1. If the final value is close to 0 the predicted label is 0 and if it is close to 1 the second label is seleted as a result. It can also include more than one node. For example, 6 nodes for the 6 corresponding lables in a dataset. Each one have a value between 0 and 1 that indicates the probability of that label. \n",
    "3. Headen layers: The layers that we do not observe.\n",
    "\n",
    "<h4>Weights</h4>\n",
    "As we said, each layer does a transformation, this transformation is implemented by the weights of each layer. weights are a bunch of numbers and sometimes are called layer parameters.\n",
    "\n",
    "<h4>Learning (Training)</h4>\n",
    "Learning means finding the best values of the weights, such that the network correctly maps the input to their associated targets.\n",
    "\n",
    "There are two essential characteristics of how deep learning learns from data: the incremental, layer-by-layer way in which increasingly complex representations are developed, and the fact that these intermediate incremental representations are learned jointly, each layer being updated to follow both the representational needs of the layer above and the needs of the layer below. Together, these two properties have made deep learning vastly more successful than previous approaches to machine learning.\n",
    "\n",
    "<h4>Loss function</h4>\n",
    "To evaluate the network and figure out how well it maps the input to output we need a loss function! This is the role of the loss function to calculate har far the final result is from the expected result. This is sometimes called cost function or objective function.\n",
    "\n",
    "<h4>optimizer</h4>\n",
    "During the learning process, the nework guesses the output, the loss function calculates the distance of the output from the expcted output. Then we give the current weights and the loss score to an optimizer that is responsible for adjusting the weights with the aim of reducing the loss in the next run of the network. \n",
    "\n",
    "<h4>Backpropagation</h4>\n",
    "This is the central algorithm in deep learning. The optimiser uses bakpropagation algorithm to adjust the weights of the network layers.\n",
    "\n",
    "\n",
    "<h4>Overview of the deep learning</h4>\n",
    "Initially, the weights of the network are assigned random values, so the network merely implements a series of random transformations. Naturally, its output is far from what it should ideally be, and the loss score is accordingly very high. But with every example the network processes, the weights are adjusted a little in the correct direction, and the loss score decreases. This is the training loop, which, repeated a suffi- cient number of times (typically tens of iterations over thousands of examples), yields weight values that minimize the loss function. A network with a minimal loss is one for which the outputs are as close as they can be to the targets: a trained network. Once again, it’s a simple mechanism that, once scaled, ends up looking like magic.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "<h3>Training</h3>\n",
    "\n",
    "Repeat these steps in a loop, until the loss seems sufficiently low:\n",
    "\n",
    "1.  Draw a batch of training samples, x, and corresponding targets, y_true.\n",
    "2. Run the model on x (a step called the forward pass) to obtain predictions, y_pred.\n",
    "3. Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true.\n",
    "4. Update all weights of the model in a way that slightly reduces the loss on this\n",
    "batch.\n",
    "\n",
    "Steps 1 to 3 are sinple. What is difficult is the step 4. how can you compute whether the coefficient should be increased or decreased, and by how much?\n",
    "\n",
    "One naive solution would be to freeze all weights in the model except the one sca- lar coefficient being considered, and try different values for this coefficient. Let’s say the initial value of the coefficient is 0.3. After the forward pass on a batch of data, the loss of the model on the batch is 0.5. If you change the coefficient’s value to 0.35 and rerun the forward pass, the loss increases to 0.6. But if you lower the coefficient to 0.25, the loss falls to 0.4. In this case, it seems that updating the coefficient by –0.05 would contribute to minimizing the loss. This would have to be repeated for all coeffi- cients in the model.\n",
    "\n",
    "But such an approach would be horribly inefficient, because you’d need to com- pute two forward passes (which are expensive) for every individual coefficient (of which there are many, usually thousands and sometimes up to millions). Thankfully, there’s a much better approach: gradient descent.\n",
    "Gradient descent is the optimization technique that powers modern neural net- works. Here’s the gist of it. All of the functions used in our models (such as dot or +) transform their input in a smooth and continuous way: if you look at z = x + y, for instance, a small change in y only results in a small change in z, and if you know the direction of the change in y, you can infer the direction of the change in z. Mathemat- ically, you’d say these functions are differentiable. If you chain together such functions, the bigger function you obtain is still differentiable. In particular, this applies to the function that maps the model’s coefficients to the loss of the model on a batch of data: a small change in the model’s coefficients results in a small, predictable change in the loss value. This enables you to use a mathematical operator called the gradient to describe how the loss varies as you move the model’s coefficients in different direc- tions. If you compute this gradient, you can use it to move the coefficients (all at once in a single update, rather than one at a time) in a direction that decreases the loss.\n",
    "    \n",
    "\n",
    "\n",
    "## Densely connected Neural network (DNN)\n",
    "\n",
    "In a densely connected neural netweork each node in a layer is connected to all nodes in the previous layer.\n",
    "\n",
    "<img src=\"images/nn.png\" width=\"50%\" />\n",
    "\n",
    "Neural network has a linear function and an activation function. The linear function calculates the input value of the nodes, and activation function calculates the out value of the nodes to the next layer.\n",
    "\n",
    "<img src=\"images/node.png\" width=\"30%\" />\n",
    "\n",
    "The b is a constant called the bias. This bias is added to the weighted sum of the all values from the previous layer. The activation function uses this value to activte the next layer. \n",
    "\n",
    "There are many activation functions with different features.\n",
    "\n",
    "\n",
    "<div style=\"margin-top:1em; display: grid; grid-template-columns: 1fr 1fr; gap: 1em;\">\n",
    "\n",
    " <div>\n",
    "  <img src=\"images/uler.png\" width=\"50%\" />\n",
    "  <p>For example, the following is called Rectified linear function:</p>\n",
    " </div>\n",
    "\n",
    " <div>\n",
    "  <img src=\"images/sigmoid.png\" width=\"50%\" />\n",
    "  <p>However, the sigmoid function is more famous. This very useful when we need our final output to be between 0 and 1.</p>\n",
    " </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "\n",
    "1.   Mean squared error\n",
    "2.   Mean absolute error\n",
    "3.   Hinge Loss\n",
    "\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "Optimizer is a function that defines how to change the wieghts and learning rate of your neural network to reduce the losses. Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.\n",
    "\n",
    "There are different optimizers. Gradient descent, stochastic gradient descent, adagrad and so. However, the best optimizer is called adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive deep classifier\n",
    "\n",
    "In this chapter we learn how to create a naive deep learner.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nn-nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a01f484bbe5ff7db30bde6dc76d21cf72867a81df06775fe6a282b1f2bc3b0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "\n",
    "## Probabilistic modeling\n",
    "Probabilistic modeling is the application of the principles of statistics to data analysis. It is one of the earliest forms of machine learning, and it’s still widely used to this day. One of the best-known algorithms in this category is the Naive Bayes algorithm.\n",
    "\n",
    "Logistic regression is the other statistic tool which is still popular these days.\n",
    "\n",
    "## Kernel methods\n",
    "Kernel methods are a group of classification algorithms, the best known of which is the Support Vector Machine (SVM).\n",
    "At the time they were developed, SVMs exhibited state-of-the-art performance on simple classification problems and were one of the few machine learning methods backed by extensive theory and amenable to serious mathematical analysis, making them well understood and easily interpretable. Because of these useful properties, SVMs became extremely popular in the field for a long time.\n",
    "\n",
    "But SVMs proved hard to scale to large datasets and didn’t provide good results for perceptual problems such as image classification. Because an SVM is a shallow method, applying an SVM to perceptual problems requires first extracting useful representations manually (a step called feature engineering), which is difficult and brittle. For instance, if you want to use an SVM to classify handwritten digits, you can’t start from the raw pixels; you should first find by hand useful representations that make the problem more tractable, like the pixel histograms mentioned earlier.\n",
    "\n",
    "## Decision trees, random forests, and gradient boosting machines\n",
    "Decision trees are flowchart-like structures that let you classify input data points or predict output values given inputs. They’re easy to visualize and interpret. Decision trees learned from data began to receive significant research interest in the 2000s, and by 2010 they were often preferred to kernel methods.\n",
    "\n",
    "The Random Forest algorithm introduced a robust, practical take on decision-tree learning that involves building a large number of specialized decision trees and then ensembling their outputs. Random forests are applicable to a wide range of problems—you could say that they’re almost always the second-best algorithm for any shallow machine learning task.\n",
    "\n",
    "The gradient boosting technique results in models that strictly outperform random forests most of the time, while having similar properties. It may be one of the best, if not the best, algorithm for dealing with nonperceptual data today.\n",
    "\n",
    "## Deep learning\n",
    "Deep learning is a specific subset of machine learning. It puts and an emphasis on learning successive layers of increasingly meaningful representations of the data. In deep learning the learning is done by a stack of neural network layers that are attached on the top of each other that try to represent the data in a form that is amenable for simple rules to make the final decisions.\n",
    "\n",
    "<h3>What makes deep learning different</h3>\n",
    "The primary reason deep learning took off so quickly is that it offered better performance for many problems. But that’s not the only reason. Deep learning also makes problem-solving much easier, because it completely automatesfeature engineering which used to be the most crucial step in a machine learning workflow before deep learning.\n",
    "\n",
    "For example, when we use neural netowrks for image processing, each layer transforms the input image to a form that is increasingly different from the original input image and more pure and informative about the final result. \n",
    "\n",
    "<img src=\"images/deep_learning.png\" width=\"50%\">\n",
    "\n",
    "The above picture illustrates that classifing hand written digits can be done by four simple rules using the last layer of a neural network, while a rule-based system may need hundreds of rules to classify digits based on the original image.\n",
    "\n",
    "These layers have been named in literature:\n",
    "\n",
    "1.   Input layer: The first layer is called the input layer. The number of nodes in the input layer is rela†ed to the number of the features in our dataset.\n",
    "2.   Output layer: The final layer is called the output layer. this layer may have only one node which indicates the final out put. For example a range between 0 and 1. If the final value is close to 0 the predicted label is 0 and if it is close to 1 the second label is seleted as a result. It can also include more than one node. For example, 6 nodes for the 6 corresponding lables in a dataset. Each one have a value between 0 and 1 that indicates the probability of that label. \n",
    "3. Headen layers: The layers that we do not observe.\n",
    "\n",
    "<h4>Weights</h4>\n",
    "As we said, each layer does a transformation, this transformation is implemented by the weights of each layer. weights are a bunch of numbers and sometimes are called layer parameters.\n",
    "\n",
    "<h4>Learning (Training)</h4>\n",
    "Learning means finding the best values of the weights, such that the network correctly maps the input to their associated targets.\n",
    "\n",
    "There are two essential characteristics of how deep learning learns from data: the incremental, layer-by-layer way in which increasingly complex representations are developed, and the fact that these intermediate incremental representations are learned jointly, each layer being updated to follow both the representational needs of the layer above and the needs of the layer below. Together, these two properties have made deep learning vastly more successful than previous approaches to machine learning.\n",
    "\n",
    "<h4>Loss function</h4>\n",
    "To evaluate the network and figure out how well it maps the input to output we need a loss function! This is the role of the loss function to calculate har far the final result is from the expected result. This is sometimes called cost function or objective function.\n",
    "\n",
    "<h4>optimizer</h4>\n",
    "During the learning process, the nework guesses the output, the loss function calculates the distance of the output from the expcted output. Then we give the current weights and the loss score to an optimizer that is responsible for adjusting the weights with the aim of reducing the loss in the next run of the network. \n",
    "\n",
    "<h4>Backpropagation</h4>\n",
    "This is the central algorithm in deep learning. The optimiser uses bakpropagation algorithm to adjust the weights of the network layers.\n",
    "\n",
    "\n",
    "<h4>Overview of the deep learning</h4>\n",
    "Initially, the weights of the network are assigned random values, so the network merely implements a series of random transformations. Naturally, its output is far from what it should ideally be, and the loss score is accordingly very high. But with every example the network processes, the weights are adjusted a little in the correct direction, and the loss score decreases. This is the training loop, which, repeated a suffi- cient number of times (typically tens of iterations over thousands of examples), yields weight values that minimize the loss function. A network with a minimal loss is one for which the outputs are as close as they can be to the targets: a trained network. Once again, it’s a simple mechanism that, once scaled, ends up looking like magic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "<h3>Training</h3>\n",
    "\n",
    "Repeat these steps in a loop, until the loss seems sufficiently low:\n",
    "\n",
    "1.  Draw a batch of training samples, x, and corresponding targets, y_true.\n",
    "2. Run the model on x (a step called the forward pass) to obtain predictions, y_pred.\n",
    "3. Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true.\n",
    "4. Update all weights of the model in a way that slightly reduces the loss on this\n",
    "batch.\n",
    "\n",
    "Steps 1 to 3 are sinple. What is difficult is the step 4. how can you compute whether the coefficient should be increased or decreased, and by how much?\n",
    "\n",
    "One naive solution would be to freeze all weights in the model except the one sca- lar coefficient being considered, and try different values for this coefficient. Let’s say the initial value of the coefficient is 0.3. After the forward pass on a batch of data, the loss of the model on the batch is 0.5. If you change the coefficient’s value to 0.35 and rerun the forward pass, the loss increases to 0.6. But if you lower the coefficient to 0.25, the loss falls to 0.4. In this case, it seems that updating the coefficient by –0.05 would contribute to minimizing the loss. This would have to be repeated for all coeffi- cients in the model.\n",
    "\n",
    "But such an approach would be horribly inefficient, because you’d need to com- pute two forward passes (which are expensive) for every individual coefficient (of which there are many, usually thousands and sometimes up to millions). Thankfully, there’s a much better approach: gradient descent.\n",
    "Gradient descent is the optimization technique that powers modern neural net- works. Here’s the gist of it. All of the functions used in our models (such as dot or +) transform their input in a smooth and continuous way: if you look at z = x + y, for instance, a small change in y only results in a small change in z, and if you know the direction of the change in y, you can infer the direction of the change in z. Mathemat- ically, you’d say these functions are differentiable. If you chain together such functions, the bigger function you obtain is still differentiable. In particular, this applies to the function that maps the model’s coefficients to the loss of the model on a batch of data: a small change in the model’s coefficients results in a small, predictable change in the loss value. This enables you to use a mathematical operator called the gradient to describe how the loss varies as you move the model’s coefficients in different direc- tions. If you compute this gradient, you can use it to move the coefficients (all at once in a single update, rather than one at a time) in a direction that decreases the loss.\n",
    "    \n",
    "\n",
    "\n",
    "## Densely connected Neural network (DNN)\n",
    "\n",
    "In a densely connected neural netweork each node in a layer is connected to all nodes in the previous layer.\n",
    "\n",
    "<img src=\"images/nn.png\" width=\"50%\" />\n",
    "\n",
    "Neural network has a linear function and an activation function. The linear function calculates the input value of the nodes, and activation function calculates the out value of the nodes to the next layer.\n",
    "\n",
    "<img src=\"images/node.png\" width=\"30%\" />\n",
    "\n",
    "The b is a constant called the bias. This bias is added to the weighted sum of the all values from the previous layer. The activation function uses this value to activte the next layer. \n",
    "\n",
    "There are many activation functions with different features.\n",
    "\n",
    "\n",
    "<div style=\"margin-top:1em; display: grid; grid-template-columns: 1fr 1fr; gap: 1em;\">\n",
    "\n",
    " <div>\n",
    "  <img src=\"images/uler.png\" width=\"50%\" />\n",
    "  <p>For example, the following is called Rectified linear function:</p>\n",
    " </div>\n",
    "\n",
    " <div>\n",
    "  <img src=\"images/sigmoid.png\" width=\"50%\" />\n",
    "  <p>However, the sigmoid function is more famous. This very useful when we need our final output to be between 0 and 1.</p>\n",
    " </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "\n",
    "1.   Mean squared error\n",
    "2.   Mean absolute error\n",
    "3.   Hinge Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Optimizer is a function that defines how to change the wieghts and learning rate of your neural network to reduce the losses. Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.\n",
    "\n",
    "There are different optimizers. Gradient descent, stochastic gradient descent, adagrad and so. However, the best optimizer is called adam.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive deep classifier\n",
    "\n",
    "In this chapter we learn how to create a naive deep learner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras \n",
    "\n",
    "Keras is an API that sits on top of Google’s TensorFlow, Microsoft Cognitive Toolkit (CNTK), and other machine learning frameworks. The goal is to have a single API to work with all of those and to make that work easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras we have some data sets that can be used to train neural networks. For example mnist data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images,train_labels),(test_images, test_labels)=fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 60000 images for traning and 10000 images for testing. Each image is a matrix of 28 x 28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images have different labels. Here is the list of label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1390a4a00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample1 = train_images[0]\n",
    "sample2 = train_images[1]\n",
    "sample3 = train_images[2]\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize = (5,5))\n",
    "ax1.imshow(sample1)\n",
    "ax2.imshow(sample2)\n",
    "ax3.imshow(sample3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The data that is used to train a neural network usually needs to be processed. For this example, we need to change the range of the value for each pixel from 0 to 255 to 0..1. This helps our NN to fit the data more easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255\n",
    "test_images = test_images / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    " Next we create the model using keras Sequential. This gives us a sequential neural network. In the following code, we create a model that has 784 input nodes. In the next layer we have 120 nodes that are densely connected by the nodes in the previous layer. In the output layer there are 10 nodes that stand for the ten labels of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(120,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10,activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the structure of the model, we need to set other parameters. The loss function, the optimizer and the metrics using the compile method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\" , loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Finally we use fit method to train our model.\n",
    "\n",
    "In the following example, we used batch size = 100, meaning that in each learning loop keras runs forward algorithm on 100 samples and then calculates the loss to update the gradient. Therefore, for 60000 samples we run the learning loop 600 times. We run this training session 10 times. Overal we update the weights 6000 times.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.8119\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4039 - accuracy: 0.8577\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.3665 - accuracy: 0.8689\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.3407 - accuracy: 0.8770\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.3233 - accuracy: 0.8824\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.3056 - accuracy: 0.8878\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2932 - accuracy: 0.8922\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2809 - accuracy: 0.8963\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2720 - accuracy: 0.8990\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2625 - accuracy: 0.9027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1396b1360>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images , train_labels , epochs=10 , batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model gets better after each batch and and each epoch. However, we can also cancel the fitting process eralier using a kind of cancellation method!\n",
    "\n",
    "The following code shows how we can use a callback function to cancel the training process when the loss value reached a specific threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 2s 2ms/step - loss: 0.5450 - accuracy: 0.8133\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4027 - accuracy: 0.8575\n",
      "Epoch 3/10\n",
      "599/600 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.8712\n",
      "Reached 0.,6 accuracy\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.3615 - accuracy: 0.8712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1396b0a90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LossCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get(\"loss\")< 0.4):\n",
    "            print(\"\\nReached 0.,6 accuracy\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "loss = LossCallback()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(120,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10,activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\" , loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])\n",
    "model.fit(train_images , train_labels , epochs=10 , batch_size=100 , callbacks=[loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "Now that we created a model that has accuracy of 90 percent on our training model, we need to evaluate it on our test data set. Because this is the accuracy on the test data set that really matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3466 - accuracy: 0.8809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3465520739555359, 0.8809000253677368]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images,test_labels , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy on the testing data set is lower, which means that our model is kind of overfitting over training data. So we should try other parameters and test the model again. For example, I think that if I reduce the value for epochs it would be a good idea to avoid overfiting. In the following code I use epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2354 - accuracy: 0.9120\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2266 - accuracy: 0.9145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f4ebbb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images , train_labels , epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3463 - accuracy: 0.8830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34626972675323486, 0.8830000162124634]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images,test_labels , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy on test data is higher afterusging epoch = 2. This process is called hyper parameter tuning. We have to use the best options and this is done by experience and knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "We can use the model the predict the label of new images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[2.2617279e-02 1.5867032e-08 9.7221428e-01 4.4797679e-07 4.7165938e-03\n",
      "  6.9269222e-11 4.0245865e-04 3.5284944e-09 8.8268334e-06 4.0095576e-05]]\n",
      "Pullover\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1307b6ef0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = test_images[20]\n",
    "predictions = model.predict(sample)\n",
    "print(predictions)\n",
    "print(labels[np.argmax(predictions)])\n",
    "fig , axes = plt.subplots(1,1,figsize=(3,3))\n",
    "axes.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 19:01:45.790094: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones(shape=(2,2))\n",
    "y = tf.constant([[1.,2.],[3.,4.]])\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[4., 6.],\n",
       "       [4., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(x,y) # dot prduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     result \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msquare(input_var)\n\u001b[1;32m      4\u001b[0m gradient \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(result, input_var)\n\u001b[0;32m----> 5\u001b[0m gradient\u001b[39m.\u001b[39;49mitems\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "input_var  = tf.random.uniform(shape=(10,1))\n",
    "with tf.GradientTape() as tape:\n",
    "    result = tf.square(input_var)\n",
    "gradient = tape.gradient(result, input_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "Two-class classification, or binary classification, is one of the most common kinds of machine learning problems. In this example, you’ll learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n",
    "\n",
    "For this example, we use IMDB dataset. Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 25000 traning sample and 25000 samples for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the word index to decode the samples to the original sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "word_index_reversed = dict([(v,k) for k,v in word_index.items()])\n",
    "def decode(sample):\n",
    "    return \" \".join([word_index_reversed.get(k-3,\"?\") for k in sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ? this is to watch save yourself an hour a bit of your life\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the trainning samples have different lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130, 450, 99, 117, 238, 109]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(sample) for sample in train_data[:15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, neural networks need samples with equal lenghts to work. Therefore, we need a way to convert samples to the vectors with the same lengths.  There are two ways to do that:\n",
    "\n",
    "1. Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, max_length), and start your model with a layer capa- ble of handling such integer tensors (the Embedding layer, which we’ll cover in detail later in the book).\n",
    "\n",
    "2. Multi-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [8, 5] into a 10,000-dimensional vec- tor that would be all 0s except for indices 8 and 5, which would be 1s. Then you could use a Dense layer, capable of handling floating-point vector data, as the first layer in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "max_length = max([max(sample) for sample in train_data])\n",
    "train_data_vectorized = np.zeros([len(train_data), max_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 9999)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(samples, dimension=100000):\n",
    "    results = np.zeros((len(samples), dimension))\n",
    "    for i, sequence in enumerate(samples):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorize_sequences(train_data)\n",
    "test_x = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the structur of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Dense(16,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(16,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why I chose 16 nodes for the first and the second layers, and 1 node for the output layer. First, I explain the thrid layer. Since our problem is a binary classification method, only one output node is enough to represent the final result. The final result of the sigmoid function is a number between 0 and 1, which is used to explain the probability of each class: a score between 0 and 1 indicating how likely the sample is to have the target “1”. The 1 is positive review and the label 0 is negative review.\n",
    "\n",
    "Having 16 units in the first layer means the weight matrix W will have shape (input_dimension, 16): the dot product with W will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector b and apply the relu operation). You can intuitively understand the dimensionality of your representation space as “how much freedom you’re allowing the model to have when learning internal representations.” Having more units (a higher-dimensional representation space) allows your model to learn more-complex representations, but it makes the model more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data).\n",
    "\n",
    "In our example, the input vector's dimension is 10000. So our first layer represents the input data of 10000 values with a 10000x16 weight mnatix. The shape of the weight matrix of the next layer is 16x16. And the final layer is 16x1.\n",
    "\n",
    "After structuring the data, we compile the neural network by setting the tuning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\" , loss=\"binary_crossentropy\" , metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, it is best to use the binary_crossentropy loss. It isn’t the only viable choice: for instance, you could use mean_squared_error. But crossentropy is usually the best choice when you’re dealing with models that output probabilities. Crossentropy is a quantity from the field of infor- mation theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.\n",
    "\n",
    "As for the choice of the optimizer, we’ll go with rmsprop, which is a usually a good default choice for virtually any problem.\n",
    "\n",
    "\n",
    "Now, We train the model for 10 epochs (10 iterations over all samples in the trainng data) in mini-batches of 512 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 19s 318ms/step - loss: 0.4404 - accuracy: 0.8273\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 17s 339ms/step - loss: 0.2528 - accuracy: 0.9135\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 16s 331ms/step - loss: 0.1962 - accuracy: 0.9310\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 16s 326ms/step - loss: 0.1663 - accuracy: 0.9409\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 16s 321ms/step - loss: 0.1449 - accuracy: 0.9489\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 16s 320ms/step - loss: 0.1278 - accuracy: 0.9559\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 16s 321ms/step - loss: 0.1149 - accuracy: 0.9605\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 18s 367ms/step - loss: 0.0986 - accuracy: 0.9662\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 20s 404ms/step - loss: 0.0914 - accuracy: 0.9695\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 15s 305ms/step - loss: 0.0819 - accuracy: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x134c55540>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x , train_label, epochs=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After traininng we can use evaluate method to test the model on new data. However, it is very common that we use a validation set to monitor the accuracy of the model during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 29s 895ms/step - loss: 0.5376 - accuracy: 0.7791 - val_loss: 0.4115 - val_accuracy: 0.8643\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 14s 478ms/step - loss: 0.3273 - accuracy: 0.9018 - val_loss: 0.3135 - val_accuracy: 0.8899\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 15s 506ms/step - loss: 0.2358 - accuracy: 0.9279 - val_loss: 0.2900 - val_accuracy: 0.8873\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 15s 504ms/step - loss: 0.1843 - accuracy: 0.9415 - val_loss: 0.2723 - val_accuracy: 0.8913\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 15s 513ms/step - loss: 0.1504 - accuracy: 0.9530 - val_loss: 0.2938 - val_accuracy: 0.8830\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 16s 536ms/step - loss: 0.1254 - accuracy: 0.9611 - val_loss: 0.3045 - val_accuracy: 0.8805\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 16s 551ms/step - loss: 0.1064 - accuracy: 0.9679 - val_loss: 0.3029 - val_accuracy: 0.8838\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 15s 514ms/step - loss: 0.0882 - accuracy: 0.9750 - val_loss: 0.3494 - val_accuracy: 0.8758\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 15s 516ms/step - loss: 0.0730 - accuracy: 0.9805 - val_loss: 0.3406 - val_accuracy: 0.8811\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 14s 463ms/step - loss: 0.0622 - accuracy: 0.9839 - val_loss: 0.3646 - val_accuracy: 0.8809\n"
     ]
    }
   ],
   "source": [
    "train_data_validation_part = train_x[:10000]\n",
    "train_data_fit_part = train_x[10000:]\n",
    "train_label_validation_part = train_label[:10000]\n",
    "train_label_fit_part = train_label[10000:]\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Dense(16,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(16,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")]\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\" , loss=\"binary_crossentropy\" , metrics=[\"accuracy\"])\n",
    "history = model.fit(train_data_fit_part , train_label_fit_part, epochs=10, batch_size=512 , validation_data=(train_data_validation_part,train_label_validation_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the hitory object for further investigation! Now, I use this history method to print the validation and loss value for validation set for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1646ed570>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "fig, axes = plt.subplots(1,1,figsize=(4,4))\n",
    "epochs = range(1, len(history_dict[\"val_accuracy\"])+1)\n",
    "axes.plot(epochs, history_dict[\"val_accuracy\"],\"bo\")\n",
    "axes.plot(epochs, history_dict[\"accuracy\"],\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy on the training set increases every epoch, while the accuracy on the validation set bounces up and down after the second epoch. This example shows us that a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. In precise terms, what you’re seeing is overfitting: after the fourth epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set. There are a range of techniques to mitigate overfitting that we will cover later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classifiction\n",
    "\n",
    "In this section, we’ll build a model to classify Reuters newswires into 46 mutually exclusive topics. Because we have many classes, this problem is an instance of multiclass classification, and because each data point should be classified into only one category, the problem is more specifically an instance of single-label multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 17:17:12.070837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2110848/2110848 [==============================] - 3s 2us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import reuters\n",
    "(train_data, train_label) , (test_data, test_label) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the IMDB dataset, the argument num_words=10000 restricts the data to the 10,000 most frequently occurring words found in the data. As with the IMDB reviews, each example is a list of integers, and you can use the reversed index in order to decode the samples.\n",
    "\n",
    "The important difference between reuters dataset and the imdb dataset is the number of classes. We have 46 classes here, so we represent the labels using one-hot encoding.  One-hot encoding is a widely used format for categorical data, also called categorical encoding. In this case, one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index.\n",
    "\n",
    "There is a builtin method to do this in keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(train_label)\n",
    "y_test = to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a stack of Dense layers like those we’ve been using, each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each layer can potentially become an information bottle- neck. In the previous example, we used 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(46, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18/18 [==============================] - 8s 279ms/step - loss: 2.7064 - accuracy: 0.5247\n",
      "Epoch 2/2\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 1.4623 - accuracy: 0.7072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b16ab90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x,y_train,epochs=2,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.486659"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the sum of allprediction for classes is larger than 1 so this is not suitable. Therefore we change our activation function for the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(46, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18/18 [==============================] - 9s 286ms/step - loss: 2.6278 - accuracy: 0.5551\n",
      "Epoch 2/2\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 1.3798 - accuracy: 0.7216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b943a00>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x,y_train,epochs=2,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 74ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_test[0:1])\n",
    "predictions.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you see that the final sum returns 1 which shows that we have used an appropriate activation function to make up a correct probability distribution for 46 classes. The maximum probability shows the label that should be chosen for the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets increase the epochs and see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 10s 410ms/step - loss: 2.7833 - accuracy: 0.5361 - val_loss: 1.8471 - val_accuracy: 0.6650\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 4s 278ms/step - loss: 1.4623 - accuracy: 0.7174 - val_loss: 1.3389 - val_accuracy: 0.7160\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 1.0569 - accuracy: 0.7795 - val_loss: 1.1380 - val_accuracy: 0.7600\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.8256 - accuracy: 0.8264 - val_loss: 1.0307 - val_accuracy: 0.7850\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.6595 - accuracy: 0.8633 - val_loss: 0.9569 - val_accuracy: 0.8020\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.5306 - accuracy: 0.8940 - val_loss: 0.9186 - val_accuracy: 0.8100\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.4260 - accuracy: 0.9127 - val_loss: 0.9295 - val_accuracy: 0.8040\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.3494 - accuracy: 0.9296 - val_loss: 0.8997 - val_accuracy: 0.8180\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.2908 - accuracy: 0.9394 - val_loss: 0.9023 - val_accuracy: 0.8180\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.2442 - accuracy: 0.9450 - val_loss: 0.8961 - val_accuracy: 0.8120\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.2085 - accuracy: 0.9493 - val_loss: 0.9059 - val_accuracy: 0.8120\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.1853 - accuracy: 0.9511 - val_loss: 0.9502 - val_accuracy: 0.8250\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 3s 189ms/step - loss: 0.1669 - accuracy: 0.9540 - val_loss: 0.9263 - val_accuracy: 0.8210\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.1512 - accuracy: 0.9555 - val_loss: 0.9414 - val_accuracy: 0.8200\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.1436 - accuracy: 0.9567 - val_loss: 1.0191 - val_accuracy: 0.8030\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 3s 191ms/step - loss: 0.1334 - accuracy: 0.9551 - val_loss: 0.9788 - val_accuracy: 0.8090\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.1315 - accuracy: 0.9560 - val_loss: 1.0236 - val_accuracy: 0.8040\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.1210 - accuracy: 0.9559 - val_loss: 1.0445 - val_accuracy: 0.8070\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.1169 - accuracy: 0.9562 - val_loss: 1.0331 - val_accuracy: 0.8100\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.1133 - accuracy: 0.9592 - val_loss: 1.0825 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(46, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_x[1000:],y_train[1000:],epochs=20,batch_size=512 , validation_data=(train_x[:1000],y_train[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see , the accuracy on the validation set increasesed until epoch 6 but then it bums up and down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of one-hot encoding, we can also use the original labels but use a loss function called sparse_categorical_crossentropy.\n",
    "\n",
    "Here’s what you should take away from this example:\n",
    "* If you’re trying to classify data points among N classes, your model should end with a Dense layer of size N.\n",
    "* In a single-label, multiclass classification problem, your model should end with a softmax activation so that it will output a probability distribution over the N output classes.\n",
    "* Categorical crossentropy is almost always the loss function you should use for such problems. It minimizes the distance between the probability distributions output by the model and the true distribution of the targets.\n",
    "* There are two ways to handle labels in multiclass classification:\n",
    " - Encoding the labels via categorical encoding (also known as one-hot encod-\n",
    "ing) and using categorical_crossentropy as a loss function\n",
    " - Encoding the labels as integers and using the sparse_categorical_cross-\n",
    "entropy loss function\n",
    "* If you need to classify data into a large number of categories, you should avoid\n",
    "creating information bottlenecks in your model due to intermediate layers that are too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.nn-nb': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1427500b84313c10956110794cf644389cd301d191854e975d703d0940fde275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
